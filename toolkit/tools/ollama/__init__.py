from fastapi import APIRouter

from asgiref.sync import async_to_sync
import os

import time
import ollama
import rich
import httpx
from os import PathLike
from typing import Any, AnyStr, Union, Optional, Sequence, Mapping, Literal
from typing_extensions import TypedDict, NotRequired
from collections.abc import AsyncIterator, Iterator
from pydantic import BaseModel, Field

from ...utils import JSONStreamingResponse

###
#
OLLAMA_HOST = os.environ.get("OLLAMA_HOST", "0.0.0.0")
OLLAMA_PORT = os.environ.get("OLLAMA_PORT", "11434")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "dolphin-llama3:8b-256k-v2.9-q8_0")

def _client(target: str = None, host: str = None, port: int = None, sync: bool = False):
    host = host or OLLAMA_HOST
    port = port or OLLAMA_PORT
    target = target or f"http://{host}:{port}"

    if sync:
        client =  ollama.Client(host=target)

    else:
        client = ollama.AsyncClient(host=target)
        #async_to_sync(client._request("GET", "/api/ps"))

    return client

_client(sync=True).generate(model=OLLAMA_MODEL)

#
###


###
#
# objects
#
class ToolFunction(BaseModel):
    """
    Function Callable by a tool
    """

    name: str = Field(default=..., title="Tool Name",
        description="Name of the tool function")

    description: Optional[str] = Field(default=None, title="Tool Description",
        description="Description of what the function does")

    parameters: Optional[Mapping[str, str]] = Field(default=None, title="Tool Parameters",
        description="List of json-schema objects for parameters the function accepts")

class ToolObj(BaseModel):
    """
    A tool callable by the model
    """

    ttype: str = Field(default="function", title="Tool Type",
        description="The type of the tool")

    function: ToolFunction = Field(default=..., title="Function",
        description="The function the tool calls")

class ToolCall(BaseModel):
    """
    The Tool calls generated by the model
    """

    tid: str = Field(default=..., title="Tool ID",
        description="ID Of the Tool Call")

    ttype: str = Field(default="function", title="Tool Type",
        description="The type of the tool.")

class OllamaOptions(BaseModel):
    #
    # https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values
    #
    # load time options
    numa: Optional[bool] = None
    num_ctx: Optional[int] = None
    num_batch: Optional[int] = None
    num_gpu: Optional[int] = None
    main_gpu: Optional[int] = None
    low_vram: Optional[bool] = None
    f16_kv: Optional[bool] = None
    logits_all: Optional[bool] = None
    vocab_only: Optional[bool] = None
    use_mmap: Optional[bool] = None
    use_mlock: Optional[bool] = None
    embedding_only: Optional[bool] = None
    num_thread: Optional[int] = None

    # runtime options
    num_keep: Optional[int] = None
    seed: Optional[int] = None
    num_predict: Optional[int] = None
    top_k: Optional[int] = None
    top_p: Optional[float] = None
    tfs_z: Optional[float] = None
    typical_p: Optional[float] = None
    repeat_last_n: Optional[int] = None
    temperature: Optional[float] = None
    repeat_penalty: Optional[float] = None
    presence_penalty: Optional[float] = None
    frequency_penalty: Optional[float] = None
    mirostat: Optional[int] = None
    mirostat_tau: Optional[float] = None
    mirostat_eta: Optional[float] = None
    penalize_newline: Optional[bool] = None
    stop: Optional[Sequence[str]] = None

class OllamaMessage(BaseModel):
    """
    Chat message.
    """

    role: Literal['user', 'assistant', 'system'] = Field(default="system", title="Role",
        description="Assumed role of the message. Response messages always has role 'assistant'.")

    content: str = Field(..., title="Content",
        description="Content of the message. Response messages contains message fragments when streaming.")

    name: Optional[str] = Field(default=None, title="Name",
        description="Name of the participant")

    tool_calls: Optional[Sequence[ToolCall]] = Field(None, title="Tool Calls",
        description="List of ToolCall objects")

    images: Optional[Sequence[Union[str, bytes]]] = Field(None, title="Images",
        description="List of image data for multimodal models")

#
###


###
#
# main router
#
OllamaRouter = APIRouter()

@OllamaRouter.get("/")
async def root() -> Mapping[str, Union[int, str]]:
    client = _client(sync=True)
    response = client._request("GET", "/api/ps").status_code

    base_url = client._client.base_url

    return {"response":response, "host":base_url.host, "port":base_url.port, "scheme":base_url.scheme}

#
###


###
#
# base functions
#
@OllamaRouter.get("/list")
async def list() -> Mapping[str, Any]:
    client = _client()
    response = await client.list()
    return response


class GenerateRequest(BaseModel):
    model: str
    prompt: Optional[str] = None
    system: Optional[str] = None
    template: Optional[str] = None
    context: Optional[Sequence[int]] = None
    stream: bool = False
    raw: bool = False
    format: Literal["", "json"] = ""
    images: Optional[Sequence[AnyStr]] = None
    options: Optional[OllamaOptions] = None
    keep_alive: Optional[Union[float, str]] = None

@OllamaRouter.post("/generate", response_model=None)
async def generate(req: GenerateRequest) -> Union[Mapping[str, Any], AsyncIterator[Mapping[str, Any]]]:
    client = _client()
    response = await client.generate(**req.dict())

    if req.stream:
        return JSONStreamingResponse(response, media_type="application/json")
    else:
        return response


class ChatRequest(BaseModel):
    model: str
    messages: Optional[Sequence[OllamaMessage]] = None
    stream: bool = False
    format: Literal["", "json"] = ""
    options: Optional[OllamaOptions] = None
    keep_alive: Optional[Union[float, str]] = None
    local: bool = False

@OllamaRouter.post("/chat", response_model=None)
async def chat(req: ChatRequest) -> Union[Mapping[str, Any], AsyncIterator[Mapping[str, Any]]]:
    client = _client()
    _req = req.dict()
    _req.pop("local")
    response = await client.chat(**_req)
    if req.stream and not req.local:
        return JSONStreamingResponse(response, media_type="application/json")
    else:
        return response


class EmbeddingRequest(BaseModel):
    model: str
    prompt: str = ""
    options: Optional[OllamaOptions] = None
    keep_alive: Optional[Union[float, str]] = None

@OllamaRouter.post("/embeddings")
async def embeddings(req: EmbeddingRequest) -> Mapping[str, Sequence[float]]:
    client = _client()
    response = await client.embeddings(**req.dict())
    return response


class PullRequest(BaseModel):
    model: str
    insecure: bool = False
    stream: bool = False

@OllamaRouter.post("/pull", response_model=None)
async def pull(req: PullRequest) -> Union[Mapping[str, Any], AsyncIterator[Mapping[str, Any]]]:
    client = _client()
    response = await client.pull(**req.dict())
    if req.stream:
        return JSONStreamingResponse(response, media_type="application/json")
    else:
        return response



class PushRequest(BaseModel):
    model: str
    insecure: bool = False
    stream: bool = False

@OllamaRouter.post("/push", response_model=None)
async def push(req: PushRequest) -> Union[Mapping[str, Any], AsyncIterator[Mapping[str, Any]]]:
    client = _client()
    response = await client.push(**req.dict())
    if req.stream:
        return JSONStreamingResponse(response, media_type="application/json")
    else:
        return response


class CreateRequest(BaseModel):
    model: str
    path: Optional[Union[str, PathLike]] = None
    modelfile: Optional[str] = None
    quantize: Optional[str] = None
    stream: bool = False

@OllamaRouter.post("/create", response_model=None)
async def create(req: CreateRequest) -> Union[Mapping[str, Any], AsyncIterator[Mapping[str, Any]]]:
    client = _client()
    response = await client.create(**req.dict())
    if req.stream:
        return JSONStreamingResponse(response, media_type="application/json")
    else:
        return response


class DeleteRequest(BaseModel):
    model: str

@OllamaRouter.post("/delete")
async def delete(req: DeleteRequest) -> Mapping[str, Any]:
    client = _client()
    response = await client.delete(**req.dict())
    return response


class CopyRequest(BaseModel):
    source: str
    destination: str

@OllamaRouter.post("/copy")
async def copy(req: CopyRequest) -> Mapping[str, Any]:
    client = _client()
    response = await client.copy(**req.dict())
    return response


class ShowRequest(BaseModel):
    model: str

@OllamaRouter.post("/show")
async def show(req: ShowRequest) -> Mapping[str, Any]:
    client = _client()
    response = await client.show(**req.dict())
    return response


#
# openai spec functions
#
#
# https://platform.openai.com/docs/api-reference/chat/create
class ChatCompletionsRequest(BaseModel):
    model: str
    messages: Sequence[OllamaMessage] = []
    frequency_penalty: Optional[float] = None
    logit_bias: Optional[Sequence[Any]] = None
    logprobs: Optional[bool] = False
    top_logprobs: Optional[int] = None
    max_tokens: Optional[int] = None
    n: Optional[int] = None
    presence_penalty: Optional[float] = None
    response_format: Optional[Mapping[str, str]] = {"type":"json_object"}
    seed: Optional[int] = None
    stop: Optional[Sequence[str]] = None
    stream: Optional[bool] = False
    stream_options: Optional[Mapping[str, Union[str, bool]]] = None
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    tools: Optional[Sequence[ToolObj]] = None
    tool_choice: Optional[Union[str, Mapping[str, str]]] = None
    user: Optional[str] = None

@OllamaRouter.post("/chat/completions", response_model=None)
async def chat_completions(req: ChatCompletionsRequest) -> Union[Mapping[str, Any], AsyncIterator[Mapping[str, Any]]]:

    options = {}
    for k in OllamaOptions.__fields__:
        if v := getattr(req, k, None):
            options[k] = v

    _id = str(time.time()) # change this latter
    _fp = "lol" # change this latter
    _usage = {"prompt_tokens":0, "completion_tokens":0, "total_tokens":0}

    if req.n is not None and req.n > 1:
        #
        # TODO
        # - stream multiple times?
        # - good luck
        # - need to somehow bundle the requests back together?
        # - this would be serial not parallel?
        # - is it possible to do parallel requests from a pool?
        # - that is a lot of work for something not needed
        #
        raise NotImplementedError("MULTIPLE CHOICES NOT SUPPORTED")

    else:
        p = {}
        for k in ChatRequest.__fields__:
            if v := getattr(req, k, None):
                p[k] = v

        rich.print(p)

        _result = await chat(ChatRequest(**p, local=True))

        if req.stream:
            def _map_return_values(obj):
                _obj = {}
                _obj["id"] = _id
                _obj["object"] = "chat.completion.chunk"
                _obj["created"] = int(time.time())
                _obj["model"] = obj["model"]
                _obj["system_fingerprint"] = _fp
                _obj["choices"] = [{"index":0, "delta":obj["message"]}]
                _obj["logprobs"] = req.logprobs
                _obj["finish_reason"] = obj.get("done_reason", None)
                _obj["usage"] = _usage

                return _obj

            async def _gen(obj):
                async for q in obj:
                    rich.print(q)
                    yield _map_return_values(q)

            result = JSONStreamingResponse(_gen(_result), media_type="application/json")

        else:
            def _map_return_values(obj):
                _obj = {}
                _obj["id"] = _id
                _obj["object"] = "chat.completion"
                _obj["created"] = int(time.time())
                _obj["model"] = obj["model"]
                _obj["system_fingerprint"] = _fp
                _obj["choices"] = [{"index":0, "message":obj["message"]}]
                _obj["logprobs"] = req.logprobs
                _obj["finish_reason"] = obj["done_reason"]
                _obj["usage"] = _usage

                return _obj

            result = _map_return_values(_result)

    return result


#
###